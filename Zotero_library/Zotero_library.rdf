<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns:z="http://www.zotero.org/namespaces/export#"
 xmlns:dcterms="http://purl.org/dc/terms/"
 xmlns:bib="http://purl.org/net/biblio#"
 xmlns:foaf="http://xmlns.com/foaf/0.1/"
 xmlns:link="http://purl.org/rss/1.0/modules/link/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:prism="http://prismstandard.org/namespaces/1.2/basic/"
 xmlns:vcard="http://nwalsh.com/rdf/vCard#">
    <bib:Article rdf:about="http://doi.apa.org/getdoi.cfm?doi=10.1037/a0031126">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf rdf:resource="urn:issn:2162-1535,%200275-3987"/>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Gebauer</foaf:surname>
                        <foaf:givenName>Line</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Kringelbach</foaf:surname>
                        <foaf:givenName>Morten L.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Vuust</foaf:surname>
                        <foaf:givenName>Peter</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_389"/>
        <dc:title>Ever-changing cycles of musical pleasure: The role of dopamine and anticipation.</dc:title>
        <dcterms:abstract>Music listening is highly pleasurable and important part of most people’s lives. Because music has no obvious importance for survival, the ubiquity of music remains puzzling and the brain processes underlying this attraction to music are not well understood. Like other rewards (such as food, sex, and money), pleasurable music activates structures in the dopaminergic reward system, but how music manages to tap into the brain’s reward system is less clear. Here we propose a novel framework for understanding musical pleasure, suggesting that music conforms to the recent concept of pleasure cycles with phases of “wanting/expectation,” “liking,” and “learning.” We argue that expectation is fundamental to musical pleasure, and that music can be experienced as pleasurable both when it fulfills and violates expectations. Dopaminergic neurons in the midbrain represent expectations and violations of expectations (prediction errors) in response to “rewards” and “alert/incentive salience signals.” We argue that the human brain treats music as an alert/incentive salience signal, and suggest that the activity of dopamine neurons represents aspects of the phases of musical expectation and musical learning, but not directly the phase of music liking. Finally, we propose a computational model for understanding musical anticipation and pleasure operationalized through the recent theory of predictive coding.</dcterms:abstract>
        <dc:date>12/2012</dc:date>
        <z:language>en</z:language>
        <z:shortTitle>Ever-changing cycles of musical pleasure</z:shortTitle>
        <z:libraryCatalog>DOI.org (Crossref)</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>http://doi.apa.org/getdoi.cfm?doi=10.1037/a0031126</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2022-01-28 14:47:47</dcterms:dateSubmitted>
        <bib:pages>152-167</bib:pages>
    </bib:Article>
    <bib:Journal rdf:about="urn:issn:2162-1535,%200275-3987">
        <prism:volume>22</prism:volume>
        <dc:title>Psychomusicology: Music, Mind, and Brain</dc:title>
        <dc:identifier>DOI 10.1037/a0031126</dc:identifier>
        <prism:number>2</prism:number>
        <dcterms:alternative>Psychomusicology: Music, Mind, and Brain</dcterms:alternative>
        <dc:identifier>ISSN 2162-1535, 0275-3987</dc:identifier>
    </bib:Journal>
    <z:Attachment rdf:about="#item_389">
        <z:itemType>attachment</z:itemType>
        <dc:title>Gebauer et al. - 2012 - Ever-changing cycles of musical pleasure The role.pdf</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>http://www.kringelbach.org/papers/MP_Gebauer2012.pdf</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2022-01-28 14:47:34</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <bib:Article rdf:about="https://www.pnas.org/doi/full/10.1073/pnas.191355898">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <prism:volume>98</prism:volume>
                <dc:title>Proceedings of the National Academy of Sciences</dc:title>
                <dc:identifier>DOI 10.1073/pnas.191355898</dc:identifier>
                <prism:number>20</prism:number>
            </bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Blood</foaf:surname>
                        <foaf:givenName>Anne J.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Zatorre</foaf:surname>
                        <foaf:givenName>Robert J.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_410"/>
        <dc:title>Intensely pleasurable responses to music correlate with activity in brain regions implicated in reward and emotion</dc:title>
        <dc:date>2001-09-25</dc:date>
        <z:libraryCatalog>pnas.org (Atypon)</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.pnas.org/doi/full/10.1073/pnas.191355898</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2022-09-12 10:14:36</dcterms:dateSubmitted>
        <dc:description>Publisher: Proceedings of the National Academy of Sciences</dc:description>
        <bib:pages>11818-11823</bib:pages>
    </bib:Article>
    <z:Attachment rdf:about="#item_410">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/410/Blood and Zatorre - 2001 - Intensely pleasurable responses to music correlate.pdf"/>
        <dc:title>Full Text PDF</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.pnas.org/doi/pdf/10.1073/pnas.191355898</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2022-09-12 10:14:37</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <bib:Article rdf:about="https://www.sciencedirect.com/science/article/pii/S1364661300018167">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf rdf:resource="urn:issn:1364-6613"/>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Zatorre</foaf:surname>
                        <foaf:givenName>Robert J.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Belin</foaf:surname>
                        <foaf:givenName>Pascal</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Penhune</foaf:surname>
                        <foaf:givenName>Virginia B.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_412"/>
        <link:link rdf:resource="#item_413"/>
        <dc:subject>
           <z:AutomaticTag><rdf:value>acoustic cues</rdf:value></z:AutomaticTag>
        </dc:subject>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>auditory processing</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>spectral sensitivity</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>temporal sensitivity</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:subject>
           <z:AutomaticTag><rdf:value>tonal patterns</rdf:value></z:AutomaticTag>
        </dc:subject>
        <dc:title>Structure and function of auditory cortex: music and speech</dc:title>
        <dcterms:abstract>We examine the evidence that speech and musical sounds exploit different acoustic cues: speech is highly dependent on rapidly changing broadband sounds, whereas tonal patterns tend to be slower, although small and precise changes in frequency are important. We argue that the auditory cortices in the two hemispheres are relatively specialized, such that temporal resolution is better in left auditory cortical areas and spectral resolution is better in right auditory cortical areas. We propose that cortical asymmetries might have developed as a general solution to the need to optimize processing of the acoustic environment in both temporal and frequency domains.</dcterms:abstract>
        <dc:date>2002-01-01</dc:date>
        <z:language>en</z:language>
        <z:shortTitle>Structure and function of auditory cortex</z:shortTitle>
        <z:libraryCatalog>ScienceDirect</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.sciencedirect.com/science/article/pii/S1364661300018167</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2022-09-12 10:15:04</dcterms:dateSubmitted>
        <bib:pages>37-46</bib:pages>
    </bib:Article>
    <bib:Journal rdf:about="urn:issn:1364-6613">
        <prism:volume>6</prism:volume>
        <dc:title>Trends in Cognitive Sciences</dc:title>
        <dc:identifier>DOI 10.1016/S1364-6613(00)01816-7</dc:identifier>
        <prism:number>1</prism:number>
        <dcterms:alternative>Trends in Cognitive Sciences</dcterms:alternative>
        <dc:identifier>ISSN 1364-6613</dc:identifier>
    </bib:Journal>
    <z:Attachment rdf:about="#item_412">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/412/Zatorre et al. - 2002 - Structure and function of auditory cortex music a.pdf"/>
        <dc:title>ScienceDirect Full Text PDF</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.sciencedirect.com/science/article/pii/S1364661300018167/pdfft?md5=8258105628903755277df839aae27aa1&amp;pid=1-s2.0-S1364661300018167-main.pdf&amp;isDTMRedir=Y</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2022-09-12 10:15:06</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <z:Attachment rdf:about="#item_413">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/413/S1364661300018167.html"/>
        <dc:title>ScienceDirect Snapshot</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.sciencedirect.com/science/article/pii/S1364661300018167</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2022-09-12 10:15:09</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>text/html</link:type>
    </z:Attachment>
    <bib:BookSection rdf:about="urn:isbn:978-0-19-263189-3%20978-0-19-263188-6">
        <z:itemType>bookSection</z:itemType>
        <dcterms:isPartOf>
            <bib:Book>
                <dcterms:isPartOf>
                    <bib:Series>
                       <dc:title>Series in affective science</dc:title>
                    </bib:Series>
                </dcterms:isPartOf>
                <dc:identifier>ISBN 978-0-19-263189-3 978-0-19-263188-6</dc:identifier>
                <dc:title>Music and emotion:  Theory and research</dc:title>
            </bib:Book>
        </dcterms:isPartOf>
        <dc:publisher>
            <foaf:Organization>
                <vcard:adr>
                    <vcard:Address>
                       <vcard:locality>New York, NY, US</vcard:locality>
                    </vcard:Address>
                </vcard:adr>
                <foaf:name>Oxford University Press</foaf:name>
            </foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Juslin</foaf:surname>
                        <foaf:givenName>Patrik N.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_415"/>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>Emotional Content</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:subject>
           <z:AutomaticTag><rdf:value>Functionalism</rdf:value></z:AutomaticTag>
        </dc:subject>
        <dc:subject>
           <z:AutomaticTag><rdf:value>Music</rdf:value></z:AutomaticTag>
        </dc:subject>
        <dc:subject>
           <z:AutomaticTag><rdf:value>Musicians</rdf:value></z:AutomaticTag>
        </dc:subject>
        <dc:subject>
           <z:AutomaticTag><rdf:value>Performance</rdf:value></z:AutomaticTag>
        </dc:subject>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>Theory Formulation</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:title>Communicating emotion in music performance: A review and a theoretical framework</dc:title>
        <dcterms:abstract>This chapter reviews research on communication of emotion in music performance, outlines a theoretical framework suitable for organizing the findings, and considers some implications for future research on music performance. Also discussed are the role of the performer in interpreting a given musical composition, the standard paradigm of emotional expression in music performance, whether emotional communication through music is indeed possible, the usage of codes (expressive cues, emotion categories, and emotion dimensions) by which performers express specific emotions, and a theoretical framework (the functionalist perspective as put forth by the author) by which to interpret emotional expression in music performance. The conclusion states that the vicarious functioning of cues allows performers to simultaneously communicate emotions to listeners in a universally accessible manner and to develop a personal expression; this communicative system therefore constitutes the marriage of the general and the individual and of the biological and the cultural. The communicative process, the author aruges, is fairly robust but has a limited information capacity, ultimately leaving it to the listeners to specify the precise &quot;meaning' of the music. (PsycInfo Database Record (c) 2020 APA, all rights reserved)</dcterms:abstract>
        <dc:date>2001</dc:date>
        <z:shortTitle>Communicating emotion in music performance</z:shortTitle>
        <z:libraryCatalog>APA PsycNet</z:libraryCatalog>
        <bib:pages>309-337</bib:pages>
    </bib:BookSection>
    <z:Attachment rdf:about="#item_415">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/415/2001-05534-008.html"/>
        <dc:title>Snapshot</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://psycnet.apa.org/record/2001-05534-008</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2022-09-12 10:17:15</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>text/html</link:type>
    </z:Attachment>
    <bib:Article rdf:about="http://www.tandfonline.com/doi/abs/10.1080/0929821042000317813">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf rdf:resource="urn:issn:0929-8215,%201744-5027"/>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Juslin</foaf:surname>
                        <foaf:givenName>Patrik N.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Laukka</foaf:surname>
                        <foaf:givenName>Petri</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_416"/>
        <dc:title>Expression, Perception, and Induction of Musical Emotions: A Review and a Questionnaire Study of Everyday Listening</dc:title>
        <dcterms:abstract>In this article, we provide an up-to-date overview of theory and research concerning expression, perception, and induction of emotion in music. We also provide a critique of this research, noting that previous studies have tended to neglect the social context of music listening. The most likely reason for this neglect, we argue, is that that most research on musical emotion has, implicitly or explicitly, taken the perspective of the musician in understanding responses to music. In contrast, we argue that a promising avenue toward a better understanding of emotional responses to music involves diary and questionnaire studies of how ordinary listeners actually use music in everyday life contexts. Accordingly, we present ﬁndings from an exploratory questionnaire study featuring 141 music listeners (between 17 and 74 years of age) that offers some novel insights. The results provide preliminary estimates of the occurrence of various emotions in listening to music, as well as clues to how music is used by listeners in a number of different emotional ways in various life contexts. These results conﬁrm that emotion is strongly related to most people’s primary motives for listening to music.</dcterms:abstract>
        <dc:date>09/2004</dc:date>
        <z:language>en</z:language>
        <z:shortTitle>Expression, Perception, and Induction of Musical Emotions</z:shortTitle>
        <z:libraryCatalog>DOI.org (Crossref)</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>http://www.tandfonline.com/doi/abs/10.1080/0929821042000317813</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2022-09-12 10:17:44</dcterms:dateSubmitted>
        <bib:pages>217-238</bib:pages>
    </bib:Article>
    <bib:Journal rdf:about="urn:issn:0929-8215,%201744-5027">
        <prism:volume>33</prism:volume>
        <dc:title>Journal of New Music Research</dc:title>
        <dc:identifier>DOI 10.1080/0929821042000317813</dc:identifier>
        <prism:number>3</prism:number>
        <dcterms:alternative>Journal of New Music Research</dcterms:alternative>
        <dc:identifier>ISSN 0929-8215, 1744-5027</dc:identifier>
    </bib:Journal>
    <z:Attachment rdf:about="#item_416">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/416/Juslin and Laukka - 2004 - Expression, Perception, and Induction of Musical E.pdf"/>
        <dc:title>Juslin and Laukka - 2004 - Expression, Perception, and Induction of Musical E.pdf</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.tandfonline.com/doi/pdf/10.1080/0929821042000317813?cookieSet=1</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2022-09-12 10:17:40</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <bib:Article rdf:about="https://www.cambridge.org/core/product/identifier/S0140525X08005293/type/journal_article">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf rdf:resource="urn:issn:0140-525X,%201469-1825"/>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Juslin</foaf:surname>
                        <foaf:givenName>Patrik N.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Västfjäll</foaf:surname>
                        <foaf:givenName>Daniel</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_418"/>
        <dc:title>Emotional responses to music: The need to consider underlying mechanisms</dc:title>
        <dcterms:abstract>Research indicates that people value music primarily because of the emotions it evokes. Yet, the notion of musical emotions remains controversial, and researchers have so far been unable to offer a satisfactory account of such emotions. We argue that the study of musical emotions has suffered from a neglect of underlying mechanisms. Speciﬁcally, researchers have studied musical emotions without regard to how they were evoked, or have assumed that the emotions must be based on the “default” mechanism for emotion induction, a cognitive appraisal. Here, we present a novel theoretical framework featuring six additional mechanisms through which music listening may induce emotions: (1) brain stem reﬂexes, (2) evaluative conditioning, (3) emotional contagion, (4) visual imagery, (5) episodic memory, and (6) musical expectancy. We propose that these mechanisms differ regarding such characteristics as their information focus, ontogenetic development, key brain regions, cultural impact, induction speed, degree of volitional inﬂuence, modularity, and dependence on musical structure. By synthesizing theory and ﬁndings from different domains, we are able to provide the ﬁrst set of hypotheses that can help researchers to distinguish among the mechanisms. We show that failure to control for the underlying mechanism may lead to inconsistent or non-interpretable ﬁndings. Thus, we argue that the new framework may guide future research and help to resolve previous disagreements in the ﬁeld. We conclude that music evokes emotions through mechanisms that are not unique to music, and that the study of musical emotions could beneﬁt the emotion ﬁeld as a whole by providing novel paradigms for emotion induction.</dcterms:abstract>
        <dc:date>10/2008</dc:date>
        <z:language>en</z:language>
        <z:shortTitle>Emotional responses to music</z:shortTitle>
        <z:libraryCatalog>DOI.org (Crossref)</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.cambridge.org/core/product/identifier/S0140525X08005293/type/journal_article</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2022-09-12 10:18:02</dcterms:dateSubmitted>
        <bib:pages>559-575</bib:pages>
    </bib:Article>
    <bib:Journal rdf:about="urn:issn:0140-525X,%201469-1825">
        <prism:volume>31</prism:volume>
        <dc:title>Behavioral and Brain Sciences</dc:title>
        <dc:identifier>DOI 10.1017/S0140525X08005293</dc:identifier>
        <prism:number>5</prism:number>
        <dcterms:alternative>Behav Brain Sci</dcterms:alternative>
        <dc:identifier>ISSN 0140-525X, 1469-1825</dc:identifier>
    </bib:Journal>
    <z:Attachment rdf:about="#item_418">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/418/Juslin and Västfjäll - 2008 - Emotional responses to music The need to consider.pdf"/>
        <dc:title>Juslin and Västfjäll - 2008 - Emotional responses to music The need to consider.pdf</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.cambridge.org/core/services/aop-cambridge-core/content/view/DEEDFFC61393C84BB417AE8B132EB34E/S0140525X08005293a.pdf/div-class-title-emotional-responses-to-music-the-need-to-consider-underlying-mechanisms-div.pdf</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2022-09-12 10:17:59</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <bib:Article rdf:about="#item_422">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <prism:volume>51</prism:volume>
                <dc:title>Canadian journal of experimental psychology = Revue canadienne de psychologie expérimentale</dc:title>
                <dc:identifier>DOI 10.1037/1196-1961.51.4.336</dc:identifier>
                <dcterms:alternative>Canadian journal of experimental psychology = Revue canadienne de psychologie expérimentale</dcterms:alternative>
            </bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Krumhansl</foaf:surname>
                        <foaf:givenName>Carol</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_423"/>
        <dc:title>An Exploratory Study of Musical Emotions</dc:title>
        <dcterms:abstract>A basic issue about musical emotions concerns whether music elicits emotional responses in listeners (the 'emotivist' position) or simply expresses emotions that listeners recognize in the music (the 'cognitivist' position). To address this, psychophysiological measures were recorded while listners heard two excerpts chosen to represent each of three emotions: sad, fear, and happy. The measures covered a fairly wide spectrum of cardiac, vascular, electrodermal, and respiratory functions. Other subjects indicated dynamic changes in emotions they experienced while listening to the music on one of four scales: sad, fear, happy, and tension. Both physiological and emotion judgements were made on a second-by-second basis. The physiological measures all showed a significant effect of music compared to the pre-music interval. A number of analyses, including correlations between physiology and emotion judgments, found significant differences among the excerpts. The sad excerpts produced the largest changes in heart rate, blood pressure, skin conductance and temperature. The fear excerpts produced the largest changes in blood transit time and amplitude. The happy excerpts produced the largest changes in the measures of respiration. These emotion-specific physiological changes only partially replicated those found for nonmusical emotions. The physiological effects of music observed generally support the emotivist view of musical emotions.</dcterms:abstract>
        <dc:date>1998-01-01</dc:date>
        <z:libraryCatalog>ResearchGate</z:libraryCatalog>
        <bib:pages>336-53</bib:pages>
    </bib:Article>
    <z:Attachment rdf:about="#item_423">
        <z:itemType>attachment</z:itemType>
        <dc:title>ResearchGate Link</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.researchgate.net/profile/Carol-Krumhansl/publication/13677174_An_Exploratory_Study_of_Musical_Emotions/links/0deec5187f7a8d0f5d000000/An-Exploratory-Study-of-Musical-Emotions.pdf</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2022-09-12 10:19:41</dcterms:dateSubmitted>
        <z:linkMode>3</z:linkMode>
    </z:Attachment>
    <bib:Article rdf:about="https://doi.org/10.1111/1467-8721.00165">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf rdf:resource="urn:issn:0963-7214"/>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Krumhansl</foaf:surname>
                        <foaf:givenName>Carol L.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_425"/>
        <dc:title>Music: A Link Between Cognition and Emotion</dc:title>
        <dcterms:abstract>Cognition and emotion are closely linked in music. The interplay between expectations and the sounded events is hypothesized to play a central role in creating musical tension and relaxation. The research summarized here is part of an ongoing program investigating how this dynamic aspect of musical emotion relates to the cognition of musical structure. Musical emotions change over time in intensity and quality, and these emotional changes covary with changes in psychophysiological measures. Perceptual studies support music-theoretic descriptions of musical structures that underlie listeners? expectations. Cross-cultural comparisons suggest that certain psychological principles of expectation are quite general, but that musical cultures emphasize these differentially. A schema of temporal organization that relates episodes of tension and relaxation to musical form and expressive aspects of musical performance is described. Finally, some results suggest that the expression of emotion in music shares properties with the expression of emotion in speech and dance.</dcterms:abstract>
        <dc:date>2002-04-01</dc:date>
        <z:shortTitle>Music</z:shortTitle>
        <z:libraryCatalog>SAGE Journals</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>https://doi.org/10.1111/1467-8721.00165</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2022-09-12 10:21:13</dcterms:dateSubmitted>
        <dc:description>Publisher: SAGE Publications Inc</dc:description>
        <bib:pages>45-50</bib:pages>
    </bib:Article>
    <bib:Journal rdf:about="urn:issn:0963-7214">
        <prism:volume>11</prism:volume>
        <dc:title>Current Directions in Psychological Science</dc:title>
        <dc:identifier>DOI 10.1111/1467-8721.00165</dc:identifier>
        <prism:number>2</prism:number>
        <dcterms:alternative>Curr Dir Psychol Sci</dcterms:alternative>
        <dc:identifier>ISSN 0963-7214</dc:identifier>
    </bib:Journal>
    <z:Attachment rdf:about="#item_425">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/425/Krumhansl - 2002 - Music A Link Between Cognition and Emotion.pdf"/>
        <dc:title>SAGE PDF Full Text</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://journals.sagepub.com/doi/pdf/10.1111/1467-8721.00165</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2022-09-12 10:21:14</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <bib:Article rdf:about="https://www.science.org/doi/10.1126/science.1231059">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf rdf:resource="urn:issn:0036-8075,%201095-9203"/>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Salimpoor</foaf:surname>
                        <foaf:givenName>Valorie N.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>van den Bosch</foaf:surname>
                        <foaf:givenName>Iris</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Kovacevic</foaf:surname>
                        <foaf:givenName>Natasa</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>McIntosh</foaf:surname>
                        <foaf:givenName>Anthony Randal</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Dagher</foaf:surname>
                        <foaf:givenName>Alain</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Zatorre</foaf:surname>
                        <foaf:givenName>Robert J.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_426"/>
        <dc:title>Interactions Between the Nucleus Accumbens and Auditory Cortices Predict Music Reward Value</dc:title>
        <dcterms:abstract>Music Was My First Love
            
              Why do human beings enjoy music?
              
                Salimpoor
                et al.
              
              (p.
              216
              ) combined behavioral economics with brain scanning to explore how a piece of music is considered rewarding to an individual when it is heard for the first time. They discovered that neural activity in the mesolimbic striatum during listening to a novel piece of music was the best predictor of the money listeners were willing to spend on buying the piece. These observations implicate sensory cortical areas in reward processing, which the authors attribute to the aesthetic nature of the judgment.
            
          , 
            Activity in the mesolimbic striatum and its interaction with auditory cortices determines the hedonic response to new music.
          , 
            We used functional magnetic resonance imaging to investigate neural processes when music gains reward value the first time it is heard. The degree of activity in the mesolimbic striatal regions, especially the nucleus accumbens, during music listening was the best predictor of the amount listeners were willing to spend on previously unheard music in an auction paradigm. Importantly, the auditory cortices, amygdala, and ventromedial prefrontal regions showed increased activity during listening conditions requiring valuation, but did not predict reward value, which was instead predicted by increasing functional connectivity of these regions with the nucleus accumbens as the reward value increased. Thus, aesthetic rewards arise from the interaction between mesolimbic reward circuitry and cortical networks involved in perceptual analysis and valuation.</dcterms:abstract>
        <dc:date>2013-04-12</dc:date>
        <z:language>en</z:language>
        <z:libraryCatalog>DOI.org (Crossref)</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.science.org/doi/10.1126/science.1231059</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2022-09-12 11:10:07</dcterms:dateSubmitted>
        <bib:pages>216-219</bib:pages>
    </bib:Article>
    <bib:Journal rdf:about="urn:issn:0036-8075,%201095-9203">
        <prism:volume>340</prism:volume>
        <dc:title>Science</dc:title>
        <dc:identifier>DOI 10.1126/science.1231059</dc:identifier>
        <prism:number>6129</prism:number>
        <dcterms:alternative>Science</dcterms:alternative>
        <dc:identifier>ISSN 0036-8075, 1095-9203</dc:identifier>
    </bib:Journal>
    <z:Attachment rdf:about="#item_426">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/426/Salimpoor et al. - 2013 - Interactions Between the Nucleus Accumbens and Aud.pdf"/>
        <dc:title>Salimpoor et al. - 2013 - Interactions Between the Nucleus Accumbens and Aud.pdf</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.science.org/doi/pdf/10.1126/science.1231059</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2022-09-12 11:10:04</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <z:Attachment rdf:about="#item_428">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/428/385432a0.pdf"/>
        <dc:title>385432a0.pdf</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.nature.com/articles/385432a0.pdf?origin=ppub</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2022-09-12 13:41:12</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <bib:Article rdf:about="https://www.pnas.org/doi/full/10.1073/pnas.1301228110">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <prism:volume>110</prism:volume>
                <dc:title>Proceedings of the National Academy of Sciences</dc:title>
                <dc:identifier>DOI 10.1073/pnas.1301228110</dc:identifier>
                <prism:number>supplement_2</prism:number>
            </bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Zatorre</foaf:surname>
                        <foaf:givenName>Robert J.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Salimpoor</foaf:surname>
                        <foaf:givenName>Valorie N.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_430"/>
        <dc:title>From perception to pleasure: Music and its neural substrates</dc:title>
        <dc:date>2013-06-18</dc:date>
        <z:shortTitle>From perception to pleasure</z:shortTitle>
        <z:libraryCatalog>pnas.org (Atypon)</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.pnas.org/doi/full/10.1073/pnas.1301228110</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2022-09-19 11:58:59</dcterms:dateSubmitted>
        <dc:description>Publisher: Proceedings of the National Academy of Sciences</dc:description>
        <bib:pages>10430-10437</bib:pages>
    </bib:Article>
    <z:Attachment rdf:about="#item_430">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/430/Zatorre and Salimpoor - 2013 - From perception to pleasure Music and its neural .pdf"/>
        <dc:title>Full Text PDF</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.pnas.org/doi/pdf/10.1073/pnas.1301228110</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2022-09-19 11:59:01</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <bib:Article rdf:about="https://www.jneurosci.org/lookup/doi/10.1523/JNEUROSCI.0727-20.2020">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf rdf:resource="urn:issn:0270-6474,%201529-2401"/>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Mas-Herrero</foaf:surname>
                        <foaf:givenName>Ernest</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Dagher</foaf:surname>
                        <foaf:givenName>Alain</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Farrés-Franch</foaf:surname>
                        <foaf:givenName>Marcel</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Zatorre</foaf:surname>
                        <foaf:givenName>Robert J.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_431"/>
        <dc:title>Unraveling the Temporal Dynamics of Reward Signals in Music-Induced Pleasure with TMS</dc:title>
        <dc:date>2021-04-28</dc:date>
        <z:language>en</z:language>
        <z:libraryCatalog>DOI.org (Crossref)</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.jneurosci.org/lookup/doi/10.1523/JNEUROSCI.0727-20.2020</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2022-09-19 11:59:40</dcterms:dateSubmitted>
        <bib:pages>3889-3899</bib:pages>
    </bib:Article>
    <bib:Journal rdf:about="urn:issn:0270-6474,%201529-2401">
        <prism:volume>41</prism:volume>
        <dc:title>The Journal of Neuroscience</dc:title>
        <dc:identifier>DOI 10.1523/JNEUROSCI.0727-20.2020</dc:identifier>
        <prism:number>17</prism:number>
        <dcterms:alternative>J. Neurosci.</dcterms:alternative>
        <dc:identifier>ISSN 0270-6474, 1529-2401</dc:identifier>
    </bib:Journal>
    <z:Attachment rdf:about="#item_431">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/431/Mas-Herrero et al. - 2021 - Unraveling the Temporal Dynamics of Reward Signals.pdf"/>
        <dc:title>Mas-Herrero et al. - 2021 - Unraveling the Temporal Dynamics of Reward Signals.pdf</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.jneurosci.org/content/jneuro/41/17/3889.full.pdf</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2022-09-19 11:59:38</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <bib:Article rdf:about="https://www.sciencedirect.com/science/article/pii/S0960982219312588">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf rdf:resource="urn:issn:0960-9822"/>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Cheung</foaf:surname>
                        <foaf:givenName>Vincent K. M.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Harrison</foaf:surname>
                        <foaf:givenName>Peter M. C.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Meyer</foaf:surname>
                        <foaf:givenName>Lars</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Pearce</foaf:surname>
                        <foaf:givenName>Marcus T.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Haynes</foaf:surname>
                        <foaf:givenName>John-Dylan</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Koelsch</foaf:surname>
                        <foaf:givenName>Stefan</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_434"/>
        <link:link rdf:resource="#item_435"/>
        <dc:subject>
           <z:AutomaticTag><rdf:value>fMRI</rdf:value></z:AutomaticTag>
        </dc:subject>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>predictive coding</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:subject>
           <z:AutomaticTag><rdf:value>aesthetics</rdf:value></z:AutomaticTag>
        </dc:subject>
        <dc:subject>
           <z:AutomaticTag><rdf:value>amygdala</rdf:value></z:AutomaticTag>
        </dc:subject>
        <dc:subject>
           <z:AutomaticTag><rdf:value>emotions</rdf:value></z:AutomaticTag>
        </dc:subject>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>information theory</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>nucleus accumbens</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:subject>
           <z:AutomaticTag><rdf:value>prediction</rdf:value></z:AutomaticTag>
        </dc:subject>
        <dc:subject>
           <z:AutomaticTag><rdf:value>reward</rdf:value></z:AutomaticTag>
        </dc:subject>
        <dc:subject>
           <z:AutomaticTag><rdf:value>syntax</rdf:value></z:AutomaticTag>
        </dc:subject>
        <dc:title>Uncertainty and Surprise Jointly Predict Musical Pleasure and Amygdala, Hippocampus, and Auditory Cortex Activity</dc:title>
        <dcterms:abstract>Listening to music often evokes intense emotions [1, 2]. Recent research suggests that musical pleasure comes from positive reward prediction errors, which arise when what is heard proves to be better than expected [3]. Central to this view is the engagement of the nucleus accumbens—a brain region that processes reward expectations—to pleasurable music and surprising musical events [4, 5, 6, 7, 8]. However, expectancy violations along multiple musical dimensions (e.g., harmony and melody) have failed to implicate the nucleus accumbens [9, 10, 11], and it is unknown how music reward value is assigned [12]. Whether changes in musical expectancy elicit pleasure has thus remained elusive [11]. Here, we demonstrate that pleasure varies nonlinearly as a function of the listener’s uncertainty when anticipating a musical event, and the surprise it evokes when it deviates from expectations. Taking Western tonal harmony as a model of musical syntax, we used a machine-learning model [13] to mathematically quantify the uncertainty and surprise of 80,000 chords in US Billboard pop songs. Behaviorally, we found that chords elicited high pleasure ratings when they deviated substantially from what the listener had expected (low uncertainty, high surprise) or, conversely, when they conformed to expectations in an uninformative context (high uncertainty, low surprise). Neurally, we found using fMRI that activity in the amygdala, hippocampus, and auditory cortex reflected this interaction, while the nucleus accumbens only reflected uncertainty. These findings challenge current neurocognitive models of music-evoked pleasure and highlight the synergistic interplay between prospective and retrospective states of expectation in the musical experience.
Video Abstract</dcterms:abstract>
        <dc:date>2019-12-02</dc:date>
        <z:language>en</z:language>
        <z:libraryCatalog>ScienceDirect</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.sciencedirect.com/science/article/pii/S0960982219312588</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2022-09-19 12:00:16</dcterms:dateSubmitted>
        <bib:pages>4084-4092.e4</bib:pages>
    </bib:Article>
    <bib:Journal rdf:about="urn:issn:0960-9822">
        <prism:volume>29</prism:volume>
        <dc:title>Current Biology</dc:title>
        <dc:identifier>DOI 10.1016/j.cub.2019.09.067</dc:identifier>
        <prism:number>23</prism:number>
        <dcterms:alternative>Current Biology</dcterms:alternative>
        <dc:identifier>ISSN 0960-9822</dc:identifier>
    </bib:Journal>
    <z:Attachment rdf:about="#item_434">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/434/Cheung et al. - 2019 - Uncertainty and Surprise Jointly Predict Musical P.pdf"/>
        <dc:title>ScienceDirect Full Text PDF</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.sciencedirect.com/science/article/pii/S0960982219312588/pdfft?md5=aacb9cc51e7bd77933932722d0e3b038&amp;pid=1-s2.0-S0960982219312588-main.pdf&amp;isDTMRedir=Y</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2022-09-19 12:00:18</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <z:Attachment rdf:about="#item_435">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/435/S0960982219312588.html"/>
        <dc:title>ScienceDirect Snapshot</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.sciencedirect.com/science/article/pii/S0960982219312588</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2022-09-19 12:00:22</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>text/html</link:type>
    </z:Attachment>
    <bib:Article rdf:about="http://www.nature.com/articles/nn.2726">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf rdf:resource="urn:issn:1097-6256,%201546-1726"/>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Salimpoor</foaf:surname>
                        <foaf:givenName>Valorie N</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Benovoy</foaf:surname>
                        <foaf:givenName>Mitchel</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Larcher</foaf:surname>
                        <foaf:givenName>Kevin</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Dagher</foaf:surname>
                        <foaf:givenName>Alain</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Zatorre</foaf:surname>
                        <foaf:givenName>Robert J</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_436"/>
        <dc:title>Anatomically distinct dopamine release during anticipation and experience of peak emotion to music</dc:title>
        <dc:date>2/2011</dc:date>
        <z:language>en</z:language>
        <z:libraryCatalog>DOI.org (Crossref)</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://www.nature.com/articles/nn.2726</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2022-09-19 12:01:01</dcterms:dateSubmitted>
        <bib:pages>257-262</bib:pages>
    </bib:Article>
    <bib:Journal rdf:about="urn:issn:1097-6256,%201546-1726">
        <prism:volume>14</prism:volume>
        <dc:title>Nature Neuroscience</dc:title>
        <dc:identifier>DOI 10.1038/nn.2726</dc:identifier>
        <prism:number>2</prism:number>
        <dcterms:alternative>Nat Neurosci</dcterms:alternative>
        <dc:identifier>ISSN 1097-6256, 1546-1726</dc:identifier>
    </bib:Journal>
    <z:Attachment rdf:about="#item_436">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/436/Salimpoor et al. - 2011 - Anatomically distinct dopamine release during anti.pdf"/>
        <dc:title>Salimpoor et al. - 2011 - Anatomically distinct dopamine release during anti.pdf</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>http://audition.ens.fr/P2web/eval2011/BT_Salimpoor2011.pdf</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2022-09-19 12:00:59</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <bib:Article rdf:about="https://doi.org/10.1177/0305735620968260">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf rdf:resource="urn:issn:0305-7356"/>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Liu</foaf:surname>
                        <foaf:givenName>Ying</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Tang</foaf:surname>
                        <foaf:givenName>Qingting</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Zhao</foaf:surname>
                        <foaf:givenName>Xingcong</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Lu</foaf:surname>
                        <foaf:givenName>Huan</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Yuan</foaf:surname>
                        <foaf:givenName>Guangjie</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Wang</foaf:surname>
                        <foaf:givenName>Gaoyuan</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Liu</foaf:surname>
                        <foaf:givenName>Guangyuan</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_439"/>
        <dc:title>Neural activation of different music styles during emotion-evoking</dc:title>
        <dcterms:abstract>Music style is tightly connected with listeners? emotional processes and neural activities. However, it remains unclear how the brain works when different music styles are processed emotionally. The current study analyzed the neural activation associated with five music styles during emotion-evoking. Twenty non-musicians participated in the functional magnetic resonance imaging (fMRI) scanning and the emotional ratings of pleasure and arousal evoked by pop, rock, jazz, folk, and classical music. Results showed that classical music was associated with the highest pleasure rating and deactivation of the corpus callosum. Rock music was associated with the highest arousal rating and deactivation of the cingulate gyrus. Pop music activated the bilateral supplementary motor areas (SMA) and the superior temporal gyrus (STG) with moderate pleasure and arousal. As the first fMRI experiment investigating the relationship between the music style and emotion, it provides neural correlates of different music styles during emotion-evoking.</dcterms:abstract>
        <dc:date>2021-11-01</dc:date>
        <z:language>en</z:language>
        <z:libraryCatalog>SAGE Journals</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>https://doi.org/10.1177/0305735620968260</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2022-09-19 12:01:21</dcterms:dateSubmitted>
        <dc:description>Publisher: SAGE Publications Ltd</dc:description>
        <bib:pages>1546-1560</bib:pages>
    </bib:Article>
    <bib:Journal rdf:about="urn:issn:0305-7356">
        <prism:volume>49</prism:volume>
        <dc:title>Psychology of Music</dc:title>
        <dc:identifier>DOI 10.1177/0305735620968260</dc:identifier>
        <prism:number>6</prism:number>
        <dc:identifier>ISSN 0305-7356</dc:identifier>
    </bib:Journal>
    <z:Attachment rdf:about="#item_439">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/439/Liu et al. - 2021 - Neural activation of different music styles during.pdf"/>
        <dc:title>SAGE PDF Full Text</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://journals.sagepub.com/doi/pdf/10.1177/0305735620968260</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2022-09-19 12:01:23</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <bib:Article rdf:about="https://journals.lww.com/neuroreport/Fulltext/2008/12030/Music_in_minor_activates_limbic_structures__a.00014.aspx">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf rdf:resource="urn:issn:0959-4965"/>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Koelsch</foaf:surname>
                        <foaf:givenName>Stefan</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Fritz</foaf:surname>
                        <foaf:givenName>Thomas</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Schlaug</foaf:surname>
                        <foaf:givenName>Gottfried</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_441"/>
        <dc:title>Amygdala activity can be modulated by unexpected chord functions during music listening</dc:title>
        <dcterms:abstract>Numerous earlier studies have investigated the cognitive processing of musical syntax with regular and irregular chord sequences. However, irregular sequences may also be perceived as unexpected, and therefore have a different emotional valence than regular sequences. We provide behavioral data showing that irregular chord functions presented in chord sequence paradigms are perceived as less pleasant than regular sequences. A reanalysis of functional MRI data showed increased blood oxygen level-dependent signal changes bilaterally in the amygdala in response to music-syntactically irregular (compared with regular) chord functions. The combined data indicate that music-syntactically irregular events elicit brain activity related to emotional processes, and that, in addition to intensely pleasurable music or highly unpleasant music, single chord functions can also modulate amygdala activity.</dcterms:abstract>
        <dc:date>December 3, 2008</dc:date>
        <z:language>en-US</z:language>
        <z:libraryCatalog>journals.lww.com</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://journals.lww.com/neuroreport/Fulltext/2008/12030/Music_in_minor_activates_limbic_structures__a.00014.aspx</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2022-09-19 12:02:33</dcterms:dateSubmitted>
        <bib:pages>1815–1819</bib:pages>
    </bib:Article>
    <bib:Journal rdf:about="urn:issn:0959-4965">
        <prism:volume>19</prism:volume>
        <dc:title>NeuroReport</dc:title>
        <dc:identifier>DOI 10.1097/WNR.0b013e32831a8722</dc:identifier>
        <prism:number>18</prism:number>
        <dc:identifier>ISSN 0959-4965</dc:identifier>
    </bib:Journal>
    <z:Attachment rdf:about="#item_441">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/441/Music_in_minor_activates_limbic_structures__a.00014.html"/>
        <dc:title>Snapshot</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://journals.lww.com/neuroreport/Fulltext/2008/12030/Music_in_minor_activates_limbic_structures__a.00014.aspx</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2022-09-19 12:02:39</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>text/html</link:type>
    </z:Attachment>
    <bib:Article rdf:about="https://linkinghub.elsevier.com/retrieve/pii/S1364661313000491">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf rdf:resource="urn:issn:13646613"/>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Chanda</foaf:surname>
                        <foaf:givenName>Mona Lisa</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Levitin</foaf:surname>
                        <foaf:givenName>Daniel J.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_442"/>
        <dc:title>The neurochemistry of music</dc:title>
        <dc:date>04/2013</dc:date>
        <z:language>en</z:language>
        <z:libraryCatalog>DOI.org (Crossref)</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://linkinghub.elsevier.com/retrieve/pii/S1364661313000491</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2022-09-19 12:04:45</dcterms:dateSubmitted>
        <bib:pages>179-193</bib:pages>
    </bib:Article>
    <bib:Journal rdf:about="urn:issn:13646613">
        <prism:volume>17</prism:volume>
        <dc:title>Trends in Cognitive Sciences</dc:title>
        <dc:identifier>DOI 10.1016/j.tics.2013.02.007</dc:identifier>
        <prism:number>4</prism:number>
        <dcterms:alternative>Trends in Cognitive Sciences</dcterms:alternative>
        <dc:identifier>ISSN 13646613</dc:identifier>
    </bib:Journal>
    <z:Attachment rdf:about="#item_442">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/442/Chanda and Levitin - 2013 - The neurochemistry of music.pdf"/>
        <dc:title>Chanda and Levitin - 2013 - The neurochemistry of music.pdf</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>http://www.downloads.imune.net/medicalbooks/Neurochemistry%20of%20music.pdf</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2022-09-19 12:04:42</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <bib:Article rdf:about="https://www.sciencedirect.com/science/article/pii/S0149763416300082">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf rdf:resource="urn:issn:0149-7634"/>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Frühholz</foaf:surname>
                        <foaf:givenName>Sascha</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Trost</foaf:surname>
                        <foaf:givenName>Wiebke</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Kotz</foaf:surname>
                        <foaf:givenName>Sonja A.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_445"/>
        <link:link rdf:resource="#item_446"/>
        <dc:subject>
           <z:AutomaticTag><rdf:value>Music</rdf:value></z:AutomaticTag>
        </dc:subject>
        <dc:subject>
           <z:AutomaticTag><rdf:value>Affect</rdf:value></z:AutomaticTag>
        </dc:subject>
        <dc:subject>
           <z:AutomaticTag><rdf:value>Auditory cortex</rdf:value></z:AutomaticTag>
        </dc:subject>
        <dc:subject>
           <z:AutomaticTag><rdf:value>Basal ganglia</rdf:value></z:AutomaticTag>
        </dc:subject>
        <dc:subject>
           <z:AutomaticTag><rdf:value>Cerebellum</rdf:value></z:AutomaticTag>
        </dc:subject>
        <dc:subject>
           <z:AutomaticTag><rdf:value>Limbic system</rdf:value></z:AutomaticTag>
        </dc:subject>
        <dc:subject>
           <z:AutomaticTag><rdf:value>Sound</rdf:value></z:AutomaticTag>
        </dc:subject>
        <dc:subject>
           <z:AutomaticTag><rdf:value>Voice</rdf:value></z:AutomaticTag>
        </dc:subject>
        <dc:title>The sound of emotions—Towards a unifying neural network perspective of affective sound processing</dc:title>
        <dcterms:abstract>Affective sounds are an integral part of the natural and social environment that shape and influence behavior across a multitude of species. In human primates, these affective sounds span a repertoire of environmental and human sounds when we vocalize or produce music. In terms of neural processing, cortical and subcortical brain areas constitute a distributed network that supports our listening experience to these affective sounds. Taking an exhaustive cross-domain view, we accordingly suggest a common neural network that facilitates the decoding of the emotional meaning from a wide source of sounds rather than a traditional view that postulates distinct neural systems for specific affective sound types. This new integrative neural network view unifies the decoding of affective valence in sounds, and ascribes differential as well as complementary functional roles to specific nodes within a common neural network. It also highlights the importance of an extended brain network beyond the central limbic and auditory brain systems engaged in the processing of affective sounds.</dcterms:abstract>
        <dc:date>2016-09-01</dc:date>
        <z:language>en</z:language>
        <z:libraryCatalog>ScienceDirect</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.sciencedirect.com/science/article/pii/S0149763416300082</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2022-09-19 12:05:22</dcterms:dateSubmitted>
        <bib:pages>96-110</bib:pages>
    </bib:Article>
    <bib:Journal rdf:about="urn:issn:0149-7634">
        <prism:volume>68</prism:volume>
        <dc:title>Neuroscience &amp; Biobehavioral Reviews</dc:title>
        <dc:identifier>DOI 10.1016/j.neubiorev.2016.05.002</dc:identifier>
        <dcterms:alternative>Neuroscience &amp; Biobehavioral Reviews</dcterms:alternative>
        <dc:identifier>ISSN 0149-7634</dc:identifier>
    </bib:Journal>
    <z:Attachment rdf:about="#item_445">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/445/Frühholz et al. - 2016 - The sound of emotions—Towards a unifying neural ne.pdf"/>
        <dc:title>ScienceDirect Full Text PDF</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.sciencedirect.com/science/article/pii/S0149763416300082/pdfft?md5=d53436296059e1aa656957e4b4415268&amp;pid=1-s2.0-S0149763416300082-main.pdf&amp;isDTMRedir=Y</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2022-09-19 12:05:24</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <z:Attachment rdf:about="#item_446">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/446/S0149763416300082.html"/>
        <dc:title>ScienceDirect Snapshot</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.sciencedirect.com/science/article/pii/S0149763416300082</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2022-09-19 12:05:28</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>text/html</link:type>
    </z:Attachment>
    <bib:Article rdf:about="https://onlinelibrary.wiley.com/doi/abs/10.1111/desc.12193">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf rdf:resource="urn:issn:1467-7687"/>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Cirelli</foaf:surname>
                        <foaf:givenName>Laura K.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Einarson</foaf:surname>
                        <foaf:givenName>Kathleen M.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Trainor</foaf:surname>
                        <foaf:givenName>Laurel J.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_449"/>
        <link:link rdf:resource="#item_448"/>
        <dc:title>Interpersonal synchrony increases prosocial behavior in infants</dc:title>
        <dcterms:abstract>Adults who move together to a shared musical beat synchronously as opposed to asynchronously are subsequently more likely to display prosocial behaviors toward each other. The development of musical behaviors during infancy has been described previously, but the social implications of such behaviors in infancy have been little studied. In Experiment 1, each of 48 14-month-old infants was held by an assistant and gently bounced to music while facing the experimenter, who bounced either in-synchrony or out-of-synchrony with the way the infant was bounced. The infants were then placed in a situation in which they had the opportunity to help the experimenter by handing objects to her that she had ‘accidently’ dropped. We found that 14-month-old infants were more likely to engage in altruistic behavior and help the experimenter after having been bounced to music in synchrony with her, compared to infants who were bounced to music asynchronously with her. The results of Experiment 2, using anti-phase bouncing, suggest that this is due to the contingency of the synchronous movements as opposed to movement symmetry. These findings support the hypothesis that interpersonal motor synchrony might be one key component of musical engagement that encourages social bonds among group members, and suggest that this motor synchrony to music may promote the very early development of altruistic behavior. A video abstract of this article can be viewed at http://www.youtube.com/watch?v=IaqWehfDm7c&amp;feature=youtu.be</dcterms:abstract>
        <dc:date>2014</dc:date>
        <z:language>en</z:language>
        <z:libraryCatalog>Wiley Online Library</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://onlinelibrary.wiley.com/doi/abs/10.1111/desc.12193</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2022-09-19 13:02:30</dcterms:dateSubmitted>
        <dc:description>_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/desc.12193</dc:description>
        <bib:pages>1003-1011</bib:pages>
    </bib:Article>
    <bib:Journal rdf:about="urn:issn:1467-7687">
        <prism:volume>17</prism:volume>
        <dc:title>Developmental Science</dc:title>
        <dc:identifier>DOI 10.1111/desc.12193</dc:identifier>
        <prism:number>6</prism:number>
        <dc:identifier>ISSN 1467-7687</dc:identifier>
    </bib:Journal>
    <z:Attachment rdf:about="#item_449">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/449/Cirelli et al. - 2014 - Interpersonal synchrony increases prosocial behavi.pdf"/>
        <dc:title>Full Text PDF</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://onlinelibrary.wiley.com/doi/pdfdirect/10.1111/desc.12193</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2022-09-19 13:02:50</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <z:Attachment rdf:about="#item_448">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/448/desc.html"/>
        <dc:title>Snapshot</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://onlinelibrary.wiley.com/doi/10.1111/desc.12193</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2022-09-19 13:02:48</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>text/html</link:type>
    </z:Attachment>
    <bib:Article rdf:about="https://doi.org/10.1093/cercor/bhaa373">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf rdf:resource="urn:issn:1047-3211"/>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Putkinen</foaf:surname>
                        <foaf:givenName>Vesa</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Nazari-Farsani</foaf:surname>
                        <foaf:givenName>Sanaz</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Seppälä</foaf:surname>
                        <foaf:givenName>Kerttu</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Karjalainen</foaf:surname>
                        <foaf:givenName>Tomi</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Sun</foaf:surname>
                        <foaf:givenName>Lihua</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Karlsson</foaf:surname>
                        <foaf:givenName>Henry K</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Hudson</foaf:surname>
                        <foaf:givenName>Matthew</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Heikkilä</foaf:surname>
                        <foaf:givenName>Timo T</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Hirvonen</foaf:surname>
                        <foaf:givenName>Jussi</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Nummenmaa</foaf:surname>
                        <foaf:givenName>Lauri</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_464"/>
        <link:link rdf:resource="#item_465"/>
        <dc:title>Decoding Music-Evoked Emotions in the Auditory and Motor Cortex</dc:title>
        <dcterms:abstract>Music can induce strong subjective experience of emotions, but it is debated whether these responses engage the same neural circuits as emotions elicited by biologically significant events. We examined the functional neural basis of music-induced emotions in a large sample (n = 102) of subjects who listened to emotionally engaging (happy, sad, fearful, and tender) pieces of instrumental music while their hemodynamic brain activity was measured with functional magnetic resonance imaging (fMRI). Ratings of the four categorical emotions and liking were used to predict hemodynamic responses in general linear model (GLM) analysis of the fMRI data. Multivariate pattern analysis (MVPA) was used to reveal discrete neural signatures of the four categories of music-induced emotions. To map neural circuits governing non-musical emotions, the subjects were scanned while viewing short emotionally evocative film clips. The GLM revealed that most emotions were associated with activity in the auditory, somatosensory, and motor cortices, cingulate gyrus, insula, and precuneus. Fear and liking also engaged the amygdala. In contrast, the film clips strongly activated limbic and cortical regions implicated in emotional processing. MVPA revealed that activity in the auditory cortex and primary motor cortices reliably discriminated the emotion categories. Our results indicate that different music-induced basic emotions have distinct representations in regions supporting auditory processing, motor control, and interoception but do not strongly rely on limbic and medial prefrontal regions critical for emotions with survival value.</dcterms:abstract>
        <dc:date>2021-05-01</dc:date>
        <z:libraryCatalog>Silverchair</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>https://doi.org/10.1093/cercor/bhaa373</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2022-10-25 06:41:17</dcterms:dateSubmitted>
        <bib:pages>2549-2560</bib:pages>
    </bib:Article>
    <bib:Journal rdf:about="urn:issn:1047-3211">
        <prism:volume>31</prism:volume>
        <dc:title>Cerebral Cortex</dc:title>
        <dc:identifier>DOI 10.1093/cercor/bhaa373</dc:identifier>
        <prism:number>5</prism:number>
        <dcterms:alternative>Cerebral Cortex</dcterms:alternative>
        <dc:identifier>ISSN 1047-3211</dc:identifier>
    </bib:Journal>
    <z:Attachment rdf:about="#item_464">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/464/Putkinen et al. - 2021 - Decoding Music-Evoked Emotions in the Auditory and.pdf"/>
        <dc:title>Full Text PDF</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://academic.oup.com/cercor/article-pdf/31/5/2549/36841240/bhaa373.pdf</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2022-10-25 06:41:22</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <z:Attachment rdf:about="#item_465">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/465/6046263.html"/>
        <dc:title>Snapshot</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://academic.oup.com/cercor/article/31/5/2549/6046263</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2022-10-25 06:41:25</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>text/html</link:type>
    </z:Attachment>
    <bib:Article rdf:about="https://www.sciencedirect.com/science/article/pii/S0010027721004339">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf rdf:resource="urn:issn:0010-0277"/>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Mori</foaf:surname>
                        <foaf:givenName>Kazuma</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_467"/>
        <link:link rdf:resource="#item_471"/>
        <dc:subject>
           <z:AutomaticTag><rdf:value>Audio</rdf:value></z:AutomaticTag>
        </dc:subject>
        <dc:subject>
           <z:AutomaticTag><rdf:value>Lyric</rdf:value></z:AutomaticTag>
        </dc:subject>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>Machine learning</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>Music information retrieval</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>Natural language processing</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:subject>
           <z:AutomaticTag><rdf:value>Peak emotion</rdf:value></z:AutomaticTag>
        </dc:subject>
        <dc:title>Decoding peak emotional responses to music from computational acoustic and lyrical features</dc:title>
        <dcterms:abstract>Music can evoke strong emotions. Research has suggested that pleasurable chills (shivering) and tears (weeping) are peak emotional responses to music. The present study examines whether computational acoustic and lyrical features can decode chills and tears. The experiment comprises 186 pieces of self-selected music to evoke emotional responses from 54 Japanese participants. Machine learning analysis with L2-norm-regularization regression revealed the decoding accuracy and specified well-defined features. In Study 1, time-series acoustic features significantly decoded emotional chills, tears, and the absence of chills or tears by using information within a few seconds before and after the onset of the three responses. The classification results showed three significant periods, indicating that complex anticipation-resolution mechanisms lead to chills and tears. Evoking chills was particularly associated with rhythm uncertainty, while evoking tears was related to harmony. Violating rhythm expectancy may have been a trigger for chills, while the harmonious overlapping of acoustic spectra may have played a role in evoking tears. In Study 2, acoustic and lyrical features from the entire piece decoded tears but not chill frequency. Mixed emotions stemming from happiness were associated with major chords, while lyric content related to sad farewells can contribute to the prediction of emotional tears, indicating that distinctive emotions in music may evoke a tear response. When considered in tandem with theoretical studies, the violation of rhythm may biologically boost both the pleasure- and fight-related physiological response of chills, whereas tears may be evolutionarily embedded in the social bonding effect of musical harmony and play a unique role in emotional regulation.</dcterms:abstract>
        <dc:date>2022-05-01</dc:date>
        <z:language>en</z:language>
        <z:libraryCatalog>ScienceDirect</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.sciencedirect.com/science/article/pii/S0010027721004339</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2022-10-31 11:20:34</dcterms:dateSubmitted>
        <bib:pages>105010</bib:pages>
    </bib:Article>
    <bib:Journal rdf:about="urn:issn:0010-0277">
        <prism:volume>222</prism:volume>
        <dc:title>Cognition</dc:title>
        <dc:identifier>DOI 10.1016/j.cognition.2021.105010</dc:identifier>
        <dcterms:alternative>Cognition</dcterms:alternative>
        <dc:identifier>ISSN 0010-0277</dc:identifier>
    </bib:Journal>
    <z:Attachment rdf:about="#item_467">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/467/Mori - 2022 - Decoding peak emotional responses to music from co.pdf"/>
        <dc:title>ScienceDirect Full Text PDF</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.sciencedirect.com/science/article/pii/S0010027721004339/pdfft?md5=daa7cfd41587fbb589c1cced3ec32aa6&amp;pid=1-s2.0-S0010027721004339-main.pdf&amp;isDTMRedir=Y</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2022-10-31 11:20:38</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <z:Attachment rdf:about="#item_471">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/471/S0010027721004339.html"/>
        <dc:title>ScienceDirect Snapshot</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.sciencedirect.com/science/article/pii/S0010027721004339</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2022-10-31 11:20:45</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>text/html</link:type>
    </z:Attachment>
    <rdf:Description rdf:about="#item_468">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <dc:title>2019 8th International Conference on Affective Computing and Intelligent Interaction (ACII)</dc:title>
                <dc:identifier>DOI 10.1109/ACII.2019.8925463</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Ma</foaf:surname>
                        <foaf:givenName>Benjamin</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Greer</foaf:surname>
                        <foaf:givenName>Timothy</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Sachs</foaf:surname>
                        <foaf:givenName>Matthew</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Habibi</foaf:surname>
                        <foaf:givenName>Assal</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Kaplan</foaf:surname>
                        <foaf:givenName>Jonas</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Narayanan</foaf:surname>
                        <foaf:givenName>Shrikanth</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_473"/>
        <link:link rdf:resource="#item_469"/>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>affective computing</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>Autoregressive processes</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>Feature extraction</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>Microsoft Windows</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>multivariate time series modeling</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>music processing</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:subject>
           <z:AutomaticTag><rdf:value>neural networks</rdf:value></z:AutomaticTag>
        </dc:subject>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>Predictive models</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:subject>
           <z:AutomaticTag><rdf:value>Timbre</rdf:value></z:AutomaticTag>
        </dc:subject>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>Time series analysis</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:title>Predicting Human-Reported Enjoyment Responses in Happy and Sad Music</dc:title>
        <dcterms:abstract>Whether in a happy mood or a sad mood, humans enjoy listening to music. In this paper, we introduce a novel method to identify auditory features that best predict listener-reported enjoyment ratings by splitting the features into qualitative feature groups, then training predictive models on these feature groups and comparing prediction performance. Using audio features that relate to dynamics, timbre, harmony, and rhythm, we predicted continuous enjoyment ratings for a set of happy and sad songs. We found that a distributed lag model with Ll regularization best predicted these responses and that timbre-related features were most relevant for predicting enjoyment ratings in happy music, while harmony-related features were most relevant to predicting enjoyment ratings in sad music. This work adds to our understanding of how music influences affective human experience.</dcterms:abstract>
        <dc:date>2019-09</dc:date>
        <z:libraryCatalog>IEEE Xplore</z:libraryCatalog>
        <dc:description>ISSN: 2156-8111</dc:description>
        <bib:pages>607-613</bib:pages>
        <bib:presentedAt>
            <bib:Conference>
                <dc:title>2019 8th International Conference on Affective Computing and Intelligent Interaction (ACII)</dc:title>
            </bib:Conference>
        </bib:presentedAt>
    </rdf:Description>
    <z:Attachment rdf:about="#item_473">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/473/8925463.html"/>
        <dc:title>IEEE Xplore Abstract Record</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://ieeexplore.ieee.org/document/8925463/?arnumber=8925463</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2022-10-31 11:20:54</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>text/html</link:type>
    </z:Attachment>
    <z:Attachment rdf:about="#item_469">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/469/Ma et al. - 2019 - Predicting Human-Reported Enjoyment Responses in H.pdf"/>
        <dc:title>IEEE Xplore Full Text PDF</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://ieeexplore.ieee.org/stampPDF/getPDF.jsp?tp=&amp;arnumber=8925463&amp;ref=aHR0cHM6Ly9pZWVleHBsb3JlLmllZWUub3JnL3N0YW1wL3N0YW1wLmpzcD9hcm51bWJlcj04OTI1NDYzJnRhZz0x</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2022-10-31 11:20:40</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <bib:Article rdf:about="http://www.tandfonline.com/doi/abs/10.1080/02699930701438145">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf rdf:resource="urn:issn:0269-9931,%201464-0600"/>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Hunter</foaf:surname>
                        <foaf:givenName>Patrick G.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Schellenberg</foaf:surname>
                        <foaf:givenName>E. Glenn</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Schimmack</foaf:surname>
                        <foaf:givenName>Ulrich</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_475"/>
        <dc:title>Mixed affective responses to music with conflicting cues</dc:title>
        <dc:date>02/2008</dc:date>
        <z:language>en</z:language>
        <z:libraryCatalog>DOI.org (Crossref)</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>http://www.tandfonline.com/doi/abs/10.1080/02699930701438145</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2022-10-31 11:51:59</dcterms:dateSubmitted>
        <bib:pages>327-352</bib:pages>
    </bib:Article>
    <bib:Journal rdf:about="urn:issn:0269-9931,%201464-0600">
        <prism:volume>22</prism:volume>
        <dc:title>Cognition &amp; Emotion</dc:title>
        <dc:identifier>DOI 10.1080/02699930701438145</dc:identifier>
        <prism:number>2</prism:number>
        <dcterms:alternative>Cognition &amp; Emotion</dcterms:alternative>
        <dc:identifier>ISSN 0269-9931, 1464-0600</dc:identifier>
    </bib:Journal>
    <z:Attachment rdf:about="#item_475">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/475/Hunter et al. - 2008 - Mixed affective responses to music with conflictin.pdf"/>
        <dc:title>Hunter et al. - 2008 - Mixed affective responses to music with conflictin.pdf</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.tandfonline.com/doi/pdf/10.1080/02699930701438145</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2022-10-31 11:51:55</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <bib:Article rdf:about="https://doi.org/10.1177/0305735607082623">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <prism:volume>36</prism:volume>
                <dc:title>Psychology of Music</dc:title>
                <dc:identifier>DOI 10.1177/0305735607082623</dc:identifier>
                <prism:number>1</prism:number>
                <dc:identifier>ISSN 0305-7356</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Kreutz</foaf:surname>
                        <foaf:givenName>Gunter</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Ott</foaf:surname>
                        <foaf:givenName>Ulrich</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Teichmann</foaf:surname>
                        <foaf:givenName>Daniel</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Osawa</foaf:surname>
                        <foaf:givenName>Patrick</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Vaitl</foaf:surname>
                        <foaf:givenName>Dieter</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_478"/>
        <dc:title>Using music to induce emotions: Influences of musical preference and                 absorption</dc:title>
        <dcterms:abstract>The present research addresses the induction of emotion during music listening in adults using categorical and dimensional theories of emotion as background. It further explores the influences of musical preference and absorption trait on induced emotion. Twenty-five excerpts of classical music representing `happiness', `sadness', `fear', `anger' and `peace' were presented individually to 99 adult participants. Participants rated the intensity of felt emotions as well as the pleasantness and arousal induced by each excerpt. Mean intensity ratings of target emotions were highest for 20 out of 25 excerpts. Pleasantness and arousal ratings led to three main clusters within the two-dimensional circumplex space. Preference for classical music significantly influenced specificity and intensity ratings across categories. Absorption trait significantly correlated with arousal ratings only. In sum, instrumental music appears effective for the induction of basic emotions in adult listeners. However, careful screening of participants in terms of their musical preferences should be mandatory.</dcterms:abstract>
        <dc:date>2008-01-01</dc:date>
        <z:shortTitle>Using music to induce emotions</z:shortTitle>
        <z:libraryCatalog>SAGE Journals</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>https://doi.org/10.1177/0305735607082623</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2022-10-31 12:33:35</dcterms:dateSubmitted>
        <dc:description>Publisher: SAGE Publications Ltd</dc:description>
        <bib:pages>101-126</bib:pages>
    </bib:Article>
    <z:Attachment rdf:about="#item_478">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/478/Kreutz et al. - 2008 - Using music to induce emotions Influences of musi.pdf"/>
        <dc:title>SAGE PDF Full Text</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://journals.sagepub.com/doi/pdf/10.1177/0305735607082623</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2022-10-31 12:33:37</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <bib:Article rdf:about="https://www.tandfonline.com/doi/full/10.1080/02699930302279">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <prism:volume>17</prism:volume>
                <dc:title>Cognition and Emotion</dc:title>
                <dc:identifier>DOI 10.1080/02699930302279</dc:identifier>
                <prism:number>1</prism:number>
                <dcterms:alternative>Cognition and Emotion</dcterms:alternative>
                <dc:identifier>ISSN 0269-9931, 1464-0600</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Gagnon</foaf:surname>
                        <foaf:givenName>Lise</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Peretz</foaf:surname>
                        <foaf:givenName>Isabelle</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_479"/>
        <dc:title>Mode and tempo relative contributions to “happy-sad” judgements in equitone melodies</dc:title>
        <dc:date>01/2003</dc:date>
        <z:language>en</z:language>
        <z:libraryCatalog>DOI.org (Crossref)</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.tandfonline.com/doi/full/10.1080/02699930302279</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2022-10-31 12:33:57</dcterms:dateSubmitted>
        <bib:pages>25-40</bib:pages>
    </bib:Article>
    <z:Attachment rdf:about="#item_479">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/479/Gagnon and Peretz - 2003 - Mode and tempo relative contributions to “happy-sa.pdf"/>
        <dc:title>Gagnon and Peretz - 2003 - Mode and tempo relative contributions to “happy-sa.pdf</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.tandfonline.com/doi/pdf/10.1080/02699930302279</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2022-10-31 12:33:54</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <bib:Article rdf:about="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3427872/">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf rdf:resource="urn:issn:1749-5016"/>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Mumford</foaf:surname>
                        <foaf:givenName>Jeanette A.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_483"/>
        <link:link rdf:resource="#item_482"/>
        <dc:title>A power calculation guide for fMRI studies</dc:title>
        <dcterms:abstract>In the past, power analyses were not that common for fMRI studies, but recent advances in power calculation techniques and software development are making power analyses much more accessible. As a result, power analyses are more commonly expected in grant applications proposing fMRI studies. Even though the software is somewhat automated, there are important decisions to be made when setting up and carrying out a power analysis. This guide provides tips on carrying out power analyses, including obtaining pilot data, defining a region of interest and other choices to help create reliable power calculations.</dcterms:abstract>
        <dc:date>2012-8</dc:date>
        <z:libraryCatalog>PubMed Central</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3427872/</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2022-11-09 11:38:37</dcterms:dateSubmitted>
        <dc:description>PMID: 22641837
PMCID: PMC3427872</dc:description>
        <bib:pages>738-742</bib:pages>
    </bib:Article>
    <bib:Journal rdf:about="urn:issn:1749-5016">
        <prism:volume>7</prism:volume>
        <dc:title>Social Cognitive and Affective Neuroscience</dc:title>
        <dc:identifier>DOI 10.1093/scan/nss059</dc:identifier>
        <prism:number>6</prism:number>
        <dcterms:alternative>Soc Cogn Affect Neurosci</dcterms:alternative>
        <dc:identifier>ISSN 1749-5016</dc:identifier>
    </bib:Journal>
    <z:Attachment rdf:about="#item_483">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/483/Mumford - 2012 - A power calculation guide for fMRI studies.pdf"/>
        <dc:title>PubMed Central Full Text PDF</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3427872/pdf/nss059.pdf</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2022-11-09 11:38:40</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <z:Attachment rdf:about="#item_482">
        <z:itemType>attachment</z:itemType>
        <dc:title>PubMed Central Link</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3427872/</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2022-11-09 11:38:37</dcterms:dateSubmitted>
        <z:linkMode>3</z:linkMode>
        <link:type>text/html</link:type>
    </z:Attachment>
    <bib:Data rdf:about="#item_491">
        <z:itemType>computerProgram</z:itemType>
        <z:programmers>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                       <foaf:surname>Microsoft</foaf:surname>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </z:programmers>
        <prism:edition>2021.1.0.1</prism:edition>
        <dc:title>Win Movie Maker</dc:title>
        <dc:date>2021</dc:date>
    </bib:Data>
    <bib:Article rdf:about="#item_492">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <prism:volume>222</prism:volume>
                <dc:title>Cognition</dc:title>
                <dc:identifier>DOI 10.1016/j.cognition.2021.105010</dc:identifier>
                <dcterms:alternative>Cognition</dcterms:alternative>
                <dc:identifier>ISSN 0010-0277</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Mori</foaf:surname>
                        <foaf:givenName>Kazuma</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_493"/>
        <link:link rdf:resource="#item_494"/>
        <dc:subject>
           <z:AutomaticTag><rdf:value>Audio</rdf:value></z:AutomaticTag>
        </dc:subject>
        <dc:subject>
           <z:AutomaticTag><rdf:value>Lyric</rdf:value></z:AutomaticTag>
        </dc:subject>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>Machine learning</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>Music information retrieval</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>Natural language processing</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:subject>
           <z:AutomaticTag><rdf:value>Peak emotion</rdf:value></z:AutomaticTag>
        </dc:subject>
        <dc:title>Decoding peak emotional responses to music from computational acoustic and lyrical features</dc:title>
        <dcterms:abstract>Music can evoke strong emotions. Research has suggested that pleasurable chills (shivering) and tears (weeping) are peak emotional responses to music. The present study examines whether computational acoustic and lyrical features can decode chills and tears. The experiment comprises 186 pieces of self-selected music to evoke emotional responses from 54 Japanese participants. Machine learning analysis with L2-norm-regularization regression revealed the decoding accuracy and specified well-defined features. In Study 1, time-series acoustic features significantly decoded emotional chills, tears, and the absence of chills or tears by using information within a few seconds before and after the onset of the three responses. The classification results showed three significant periods, indicating that complex anticipation-resolution mechanisms lead to chills and tears. Evoking chills was particularly associated with rhythm uncertainty, while evoking tears was related to harmony. Violating rhythm expectancy may have been a trigger for chills, while the harmonious overlapping of acoustic spectra may have played a role in evoking tears. In Study 2, acoustic and lyrical features from the entire piece decoded tears but not chill frequency. Mixed emotions stemming from happiness were associated with major chords, while lyric content related to sad farewells can contribute to the prediction of emotional tears, indicating that distinctive emotions in music may evoke a tear response. When considered in tandem with theoretical studies, the violation of rhythm may biologically boost both the pleasure- and fight-related physiological response of chills, whereas tears may be evolutionarily embedded in the social bonding effect of musical harmony and play a unique role in emotional regulation.</dcterms:abstract>
        <dc:date>2022-05-01</dc:date>
        <z:language>en</z:language>
        <z:libraryCatalog>ScienceDirect</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.sciencedirect.com/science/article/pii/S0010027721004339</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2023-01-08 15:35:12</dcterms:dateSubmitted>
        <bib:pages>105010</bib:pages>
    </bib:Article>
    <z:Attachment rdf:about="#item_493">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/493/Mori - 2022 - Decoding peak emotional responses to music from co.pdf"/>
        <dc:title>ScienceDirect Full Text PDF</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.sciencedirect.com/science/article/pii/S0010027721004339/pdfft?md5=daa7cfd41587fbb589c1cced3ec32aa6&amp;pid=1-s2.0-S0010027721004339-main.pdf&amp;isDTMRedir=Y</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2023-01-08 15:35:15</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <z:Attachment rdf:about="#item_494">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/494/S0010027721004339.html"/>
        <dc:title>ScienceDirect Snapshot</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.sciencedirect.com/science/article/pii/S0010027721004339</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2023-01-08 15:35:18</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>text/html</link:type>
    </z:Attachment>
    <bib:Article rdf:about="#item_495">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf>
           <bib:Journal></bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Thomasen</foaf:surname>
                        <foaf:givenName>Manuela</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_496"/>
        <dc:title>The emotional content in music perception</dc:title>
        <dcterms:abstract>The pleasurable feelings you experience when listening to music can be based on your music preference, 
how good you are at predicting where the music is going and the alignment we make with the emotional 
state in the musical piece. By getting a better understanding on how we respond to music and which 
musical characteristics that evoke emotions, we can find ways to apply it as a tool in different areas. To 
look at the relationship between the emotional response of the listener and the songs mode, a study was 
conducted. Participants (N=33) listened to 8 song clips in major or minor mode and with or without 
lyrics. After each clip participants reported om their emotional response on a valence scale, ranging 
from negative (-5) to positive (+5). Trough linear mixed effects analysis it was found that responses to 
songs in major mode was rated more positive compared to songs in minor mode, the difference was 
significant. Further an additive effect of lyrics was found, for both songs in minor and major mode the 
emotional response to songs with lyrics were more positive compared to songs without lyrics. For a 
more reliable results regarding lyrics in the songs, you could look into how the sentiment score of the 
lyrics effect the emotional response.</dcterms:abstract>
        <dc:date>10-02-2022</dc:date>
    </bib:Article>
    <z:Attachment rdf:about="#item_496">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/496/Thomasen - 2022 - The emotional content in music perception.pdf"/>
        <dc:title>Thomasen - 2022 - The emotional content in music perception.pdf</dc:title>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <bib:Book rdf:about="#item_497">
        <z:itemType>book</z:itemType>
        <dc:publisher>
            <foaf:Organization>
               <foaf:name>CreateSpace</foaf:name>
            </foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Van Rossum</foaf:surname>
                        <foaf:givenName>Guido</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Drake</foaf:surname>
                        <foaf:givenName>Fred L.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dc:title>Python 3 Reference Manual</dc:title>
        <dc:date>2009</dc:date>
    </bib:Book>
    <bib:Article rdf:about="https://ojs.aaai.org/index.php/ICWSM/article/view/14550">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf rdf:resource="urn:issn:2334-0770"/>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Hutto</foaf:surname>
                        <foaf:givenName>C.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Gilbert</foaf:surname>
                        <foaf:givenName>Eric</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_500"/>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>Human Centered Computing</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:title>VADER: A Parsimonious Rule-Based Model for Sentiment Analysis of Social Media Text</dc:title>
        <dcterms:abstract>The inherent nature of social media content poses serious challenges to practical applications of sentiment analysis. We present VADER, a simple rule-based model for general sentiment analysis, and compare its effectiveness to eleven typical state-of-practice benchmarks including LIWC, ANEW, the General Inquirer, SentiWordNet, and machine learning oriented techniques relying on Naive Bayes, Maximum Entropy, and Support Vector Machine (SVM) algorithms. Using a combination of qualitative and quantitative methods, we first construct and empirically validate a gold-standard list of lexical features (along with their associated sentiment intensity measures) which are specifically attuned to sentiment in microblog-like contexts. We then combine these lexical features with consideration for five general rules that embody grammatical and syntactical conventions for expressing and emphasizing sentiment intensity. Interestingly, using our parsimonious rule-based model to assess the sentiment of tweets, we find that VADER outperforms individual human raters (F1 Classification Accuracy = 0.96 and 0.84, respectively), and generalizes more favorably across contexts than any of our benchmarks.</dcterms:abstract>
        <dc:date>2014-05-16</dc:date>
        <z:language>en</z:language>
        <z:shortTitle>VADER</z:shortTitle>
        <z:libraryCatalog>ojs.aaai.org</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://ojs.aaai.org/index.php/ICWSM/article/view/14550</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2023-01-09 14:48:39</dcterms:dateSubmitted>
        <dc:rights>Copyright (c) 2021 Proceedings of the International AAAI Conference on Web and Social Media</dc:rights>
        <dc:description>Number: 1</dc:description>
        <bib:pages>216-225</bib:pages>
    </bib:Article>
    <bib:Journal rdf:about="urn:issn:2334-0770">
        <prism:volume>8</prism:volume>
        <dc:title>Proceedings of the International AAAI Conference on Web and Social Media</dc:title>
        <dc:identifier>DOI 10.1609/icwsm.v8i1.14550</dc:identifier>
        <prism:number>1</prism:number>
        <dc:identifier>ISSN 2334-0770</dc:identifier>
    </bib:Journal>
    <z:Attachment rdf:about="#item_500">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/500/Hutto and Gilbert - 2014 - VADER A Parsimonious Rule-Based Model for Sentime.pdf"/>
        <dc:title>Full Text PDF</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://ojs.aaai.org/index.php/ICWSM/article/download/14550/14399</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2023-01-09 14:48:46</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <bib:Document rdf:about="https://www.geeksforgeeks.org/python-sentiment-analysis-using-vader/">
        <z:itemType>blogPost</z:itemType>
        <dcterms:isPartOf>
           <z:Blog><dc:title>GeeksforGeeks</dc:title></z:Blog>
        </dcterms:isPartOf>
        <link:link rdf:resource="#item_502"/>
        <dc:title>Python | Sentiment Analysis using VADER</dc:title>
        <dcterms:abstract>A Computer Science portal for geeks. It contains well written, well thought and well explained computer science and programming articles, quizzes and practice/competitive programming/company interview Questions.</dcterms:abstract>
        <dc:date>2019-01-23T15:00:42+00:00</dc:date>
        <z:language>en-us</z:language>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.geeksforgeeks.org/python-sentiment-analysis-using-vader/</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2023-01-09 15:18:55</dcterms:dateSubmitted>
        <dc:description>Section: Python</dc:description>
    </bib:Document>
    <z:Attachment rdf:about="#item_502">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/502/python-sentiment-analysis-using-vader.html"/>
        <dc:title>Snapshot</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.geeksforgeeks.org/python-sentiment-analysis-using-vader/</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2023-01-09 15:19:05</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>text/html</link:type>
    </z:Attachment>
    <bib:Article rdf:about="https://www.sciencedirect.com/science/article/pii/S0165027002001218">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf rdf:resource="urn:issn:0165-0270"/>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Desmond</foaf:surname>
                        <foaf:givenName>John E</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Glover</foaf:surname>
                        <foaf:givenName>Gary H</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_504"/>
        <link:link rdf:resource="#item_505"/>
        <dc:subject>
           <z:AutomaticTag><rdf:value>fMRI</rdf:value></z:AutomaticTag>
        </dc:subject>
        <dc:subject>
           <z:AutomaticTag><rdf:value>Neuroimaging</rdf:value></z:AutomaticTag>
        </dc:subject>
        <dc:subject>
           <z:AutomaticTag><rdf:value>Power</rdf:value></z:AutomaticTag>
        </dc:subject>
        <dc:subject>
           <z:AutomaticTag><rdf:value>Sample size</rdf:value></z:AutomaticTag>
        </dc:subject>
        <dc:subject>
           <z:AutomaticTag><rdf:value>Statistics</rdf:value></z:AutomaticTag>
        </dc:subject>
        <dc:title>Estimating sample size in functional MRI (fMRI) neuroimaging studies: Statistical power analyses</dc:title>
        <dcterms:abstract>Estimation of statistical power in functional MRI (fMRI) requires knowledge of the expected percent signal change between two conditions as well as estimates of the variability in percent signal change. Variability can be divided into intra-subject variability, reflecting noise within the time series, and inter-subject variability, reflecting subject-to-subject differences in activation. The purpose of this study was to obtain estimates of percent signal change and the two sources of variability from fMRI data, and then use these parameter estimates in simulation experiments in order to generate power curves. Of interest from these simulations were conclusions concerning how many subjects are needed and how many time points within a scan are optimal in an fMRI study of cognitive function. Intra-subject variability was estimated from resting conditions, and inter-subject variability and percent signal change were estimated from verbal working memory data. Simulations derived from these parameters illustrate how percent signal change, intra- and inter-subject variability, and number of time points affect power. An empirical test experiment, using fMRI data acquired during somatosensory stimulation, showed good correspondence between the simulation-based power predictions and the power observed within somatosensory regions of interest. Our analyses suggested that for a liberal threshold of 0.05, about 12 subjects were required to achieve 80% power at the single voxel level for typical activations. At more realistic thresholds, that approach those used after correcting for multiple comparisons, the number of subjects doubled to maintain this level of power.</dcterms:abstract>
        <dc:date>2002-08-30</dc:date>
        <z:language>en</z:language>
        <z:shortTitle>Estimating sample size in functional MRI (fMRI) neuroimaging studies</z:shortTitle>
        <z:libraryCatalog>ScienceDirect</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.sciencedirect.com/science/article/pii/S0165027002001218</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2023-01-27 11:12:43</dcterms:dateSubmitted>
        <bib:pages>115-128</bib:pages>
    </bib:Article>
    <bib:Journal rdf:about="urn:issn:0165-0270">
        <prism:volume>118</prism:volume>
        <dc:title>Journal of Neuroscience Methods</dc:title>
        <dc:identifier>DOI 10.1016/S0165-0270(02)00121-8</dc:identifier>
        <prism:number>2</prism:number>
        <dcterms:alternative>Journal of Neuroscience Methods</dcterms:alternative>
        <dc:identifier>ISSN 0165-0270</dc:identifier>
    </bib:Journal>
    <z:Attachment rdf:about="#item_504">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/504/Desmond and Glover - 2002 - Estimating sample size in functional MRI (fMRI) ne.pdf"/>
        <dc:title>ScienceDirect Full Text PDF</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.sciencedirect.com/science/article/pii/S0165027002001218/pdfft?md5=0b69016506672b784c164795a0b22341&amp;pid=1-s2.0-S0165027002001218-main.pdf&amp;isDTMRedir=Y</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2023-01-27 11:12:46</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <z:Attachment rdf:about="#item_505">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/505/S0165027002001218.html"/>
        <dc:title>ScienceDirect Snapshot</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.sciencedirect.com/science/article/pii/S0165027002001218</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2023-01-27 11:12:50</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>text/html</link:type>
    </z:Attachment>
    <bib:Article rdf:about="http://www.hindawi.com/journals/cin/2016/2094601/">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf rdf:resource="urn:issn:1687-5265,%201687-5273"/>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Pajula</foaf:surname>
                        <foaf:givenName>Juha</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Tohka</foaf:surname>
                        <foaf:givenName>Jussi</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_506"/>
        <dc:title>How Many Is Enough? Effect of Sample Size in Inter-Subject Correlation Analysis of fMRI</dc:title>
        <dcterms:abstract>Inter-subject correlation (ISC) is a widely used method for analyzing functional magnetic resonance imaging (fMRI) data acquired during naturalistic stimuli. A challenge in ISC analysis is to define the required sample size in the way that the results are reliable. We studied the effect of the sample size on the reliability of ISC analysis and additionally addressed the following question: How many subjects are needed for the ISC statistics to converge to the ISC statistics obtained using a large sample? The study was realized using a large block design data set of 130 subjects. We performed a split-half resampling based analysis repeatedly sampling two nonoverlapping subsets of 10–65 subjects and comparing the ISC maps between the independent subject sets. Our findings suggested that with 20 subjects, on average, the ISC statistics had converged close to a large sample ISC statistic with 130 subjects. However, the split-half reliability of unthresholded and thresholded ISC maps improved notably when the number of subjects was increased from 20 to 30 or more.</dcterms:abstract>
        <dc:date>2016</dc:date>
        <z:language>en</z:language>
        <z:shortTitle>How Many Is Enough?</z:shortTitle>
        <z:libraryCatalog>DOI.org (Crossref)</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>http://www.hindawi.com/journals/cin/2016/2094601/</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2023-01-27 12:18:14</dcterms:dateSubmitted>
        <bib:pages>1-10</bib:pages>
    </bib:Article>
    <bib:Journal rdf:about="urn:issn:1687-5265,%201687-5273">
        <prism:volume>2016</prism:volume>
        <dc:title>Computational Intelligence and Neuroscience</dc:title>
        <dc:identifier>DOI 10.1155/2016/2094601</dc:identifier>
        <dcterms:alternative>Computational Intelligence and Neuroscience</dcterms:alternative>
        <dc:identifier>ISSN 1687-5265, 1687-5273</dc:identifier>
    </bib:Journal>
    <z:Attachment rdf:about="#item_506">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/506/Pajula and Tohka - 2016 - How Many Is Enough Effect of Sample Size in Inter.pdf"/>
        <dc:title>Pajula and Tohka - 2016 - How Many Is Enough Effect of Sample Size in Inter.pdf</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://dl.acm.org/doi/pdf/10.1155/2016/2094601</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2023-01-27 12:18:09</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
</rdf:RDF>
